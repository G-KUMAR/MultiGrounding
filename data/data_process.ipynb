{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets pre-processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide this notebook to enable pre-processing all datasets into a unified format. Please follow the instructions for the dataset you want to use below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we're in the correct directory\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "if cwd.endswith('data'):\n",
    "    os.chdir('../')\n",
    "cwd = os.getcwd()\n",
    "print(cwd) # This should be the root of the repository e.g. the `MultiGrounding` folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Genome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and prepare Visual Genome \n",
    "\n",
    "If you do not have the VisualGenome dataset yet:\n",
    "1. download v1.2 images and region descriptions from [VisualGenome](https://visualgenome.org/api/v0/api_home.html)\n",
    "2. put all images under folder: `VG_Images`\n",
    "\n",
    "Otherwise, you can remove the directory `VG_Images` and create a symbolic link from your VisualGenome image directory (e.g. `/datasets/VisualGenome/`) to the `VG_Images` folder as follows:\n",
    "`ln -s /datasets/VisualGenome/ VG_Images`\n",
    "\n",
    "Then:\n",
    "1. put [region_descriptions.json](https://visualgenome.org/static/data/dataset/region_descriptions.json.zip) under folder: `VG_Annotations`\n",
    "2. make sure `imgs_data.pickle` (already included) is under `VG_Annotations`\n",
    "3. make sure `data_splits.pickle` (already included) is under `VG_Splits`\n",
    " \n",
    "NB: For fair comparison with SOTA\n",
    "1. we make sure train split doesn't have overlap with any test split of other datasets\n",
    "2. we take val set of coco as test split of vg and the rest of vg as train split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process Visual Genome "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "from data.data_utils import Data\n",
    "data_saver = Data()\n",
    "data_saver(raw_path='./data/visual_genome/',save_path='./data/visual_genome_pp/',name='visual_genome',store_lmdb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flickr30K Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and prepare Flickr30K Entities\n",
    "\n",
    "1. Clone the master branch from [https://github.com/BryanPlummer/flickr30k_entities](https://github.com/BryanPlummer/flickr30k_entities)\n",
    "2. Copy annotations.zip, in the `Flickr30k_Entities` folder and unzip it, this should generate and fill the subfolders `Sentences` and `Annotations`\n",
    "3. Put `train.txt`, `test.txt`, and `val.txt` in the `Flickr30k_Splits` folder\n",
    "4. Download flickr30k-images from [http://hockenmaier.cs.illinois.edu/DenotationGraph/](http://hockenmaier.cs.illinois.edu/DenotationGraph/) and extract the zip file, rename the folder\n",
    "5. Put all images in `Flickr30k_Images` folder\n",
    "6. Put folders `Flickr30k_Entities`, `Flickr30k_Images`, `Flickr30k_Splits` under `./data/flickr30k/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "from data.data_utils import Data\n",
    "data_saver = Data()\n",
    "data_saver(raw_path='./data/flickr30k/',save_path='./data/flickr30k_pp/',name='flickr30k_entities',store_lmdb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReferIt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and prepare ReferIt\n",
    "\n",
    "1. Download the `refclef` split from [http://bvisionweb1.cs.unc.edu/licheng/referit/data/refclef.zip](http://bvisionweb1.cs.unc.edu/licheng/referit/data/refclef.zip)\n",
    "2. Download the cleaned `refclef` images from [http://bvisionweb1.cs.unc.edu/licheng/referit/data/images/saiapr_tc-12.zip](http://bvisionweb1.cs.unc.edu/licheng/referit/data/images/saiapr_tc-12.zip)\n",
    "3. Unzip files of `refclef.zip` to `ReferIt_Splits`, not as a subfolder `refclef` in `ReferIt_Splits` but with the files directly in the folder `ReferIt_Splits`\n",
    "4. unzip files of `saiapr_tc-12.zip` to `ReferIt_Images`, not as a subfolder `saiapr_tc-12` in `ReferIt_Images` but with the folders `00` to `40` directly in the folder `ReferIt_Images`\n",
    "5. Unzip files of `RefClef_Captions.tgz` (already included), this should create and fill the folder `ReferClef_Captions`\n",
    "\n",
    "Note: you can choose to download and process RefCOCO splits, but we used RefClef under \"UNC\" split (an established split in the area) and already included proper \"image_id\"s in the repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "from data.data_utils import Data\n",
    "data_saver = Data()\n",
    "data_saver(raw_path='./data/referit/',save_path='./data/referit_pp/',name='referit',store_lmdb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MS-COCO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and prepare MS-COCO\n",
    "\n",
    "1. Clone and build python API of COCO dataset from [https://github.com/cocodataset/cocoapi/tree/master/PythonAPI](https://github.com/cocodataset/cocoapi/tree/master/PythonAPI) if not already installed\n",
    "2. Download coco train/val images and annotations ([train](http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
    ") and [test](http://images.cocodataset.org/annotations/image_info_test2014.zip\n",
    ")) from [http://cocodataset.org/#download](http://cocodataset.org/#download)\n",
    "3. Unzip all splits and put all images in one folder named: `COCO_Images`\n",
    "4. Unzip annotation files and put all files under them in `COCO_Annotations`\n",
    "\n",
    "\n",
    "Note: we used version 2014 in our evaluations\n",
    "\n",
    "Note: using 'gsutil' speeds up the process of downloading images (instructions available at [http://cocodataset.org/#download](http://cocodataset.org/#download))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "from data.data_utils import Data\n",
    "data_saver = Data()\n",
    "data_saver(raw_path='./data/coco/',save_path='./data/coco_pp/',name='coco',store_lmdb=True,version='2014')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
